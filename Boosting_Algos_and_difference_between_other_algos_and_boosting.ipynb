{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Boosting Algos and difference between other algos and boosting.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOS3nS1nq6dhSkbBkKrpPF5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshaypurohit177/Deep-Learning-Basics/blob/master/Boosting_Algos_and_difference_between_other_algos_and_boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxXt_MnQuhia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u6hZb1uwiVu",
        "colab_type": "text"
      },
      "source": [
        "An ensemble is just a collection of predictors which come together (e.g. mean of all predictions) to give a final prediction. The reason we use ensembles is that many different predictors trying to predict same target variable will perform a better job than any single predictor alone. Ensembling techniques are further classified into Bagging and Boosting.\n",
        "\n",
        "\n",
        "**Bagging is a simple ensembling technique in which we build many independent predictors/models/learners and combine them using some model averaging techniques. (e.g. weighted average, majority vote or normal average)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uPZq2ygxR1o",
        "colab_type": "text"
      },
      "source": [
        "We typically take random sub-sample/bootstrap of data for each model, so that all the models are little different from each other. Each observation is chosen with replacement to be used as input for each of the model. So, each model will have different observations based on the bootstrap process. Because this technique takes many uncorrelated learners to make a final model, it reduces error by reducing variance. Example of bagging ensemble is Random Forest models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OZpBkLivpQ6",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CeTUz2ZuizI",
        "colab_type": "text"
      },
      "source": [
        "A Brief explanation on difference between Bagging/boosting and other techniques\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1400/1*8T4HEjzHto_V8PrEFLkd9A.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWE0un2DxIKQ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnpegliQxh_v",
        "colab_type": "text"
      },
      "source": [
        "Boosting is an ensemble technique in which the predictors are not made independently, but sequentially.\n",
        "\n",
        "---\n",
        "\n",
        "This technique employs the logic in which the subsequent predictors learn from the mistakes of the previous predictors. Therefore, the observations have an unequal probability of appearing in subsequent models and ones with the highest error appear most. (So the observations are not chosen based on the bootstrap process, but based on the error). The predictors can be chosen from a range of models like decision trees, regressors, classifiers etc. Because new predictors are learning from mistakes committed by previous predictors, it takes less time/iterations to reach close to actual predictions. But we have to choose the stopping criteria carefully or it could lead to overfitting on training data. Gradient Boosting is an example of boosting algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TikhPhytxyg7",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/1400/1*PaXJ8HCYE9r2MgiZ32TQ2A.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90niTlag0SMl",
        "colab_type": "text"
      },
      "source": [
        "In summary,\n",
        "• We first model data with simple models and analyze data for errors.\n",
        "\n",
        "• These errors signify data points that are difficult to fit by a simple model.\n",
        "\n",
        "• Then for later models, we particularly focus on those hard to fit data to get them right.\n",
        "\n",
        "• In the end, we combine all the predictors by giving some weights to each predictor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgGmjFLeyER_",
        "colab_type": "text"
      },
      "source": [
        "https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOT2OwQp1FPt",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Yotube links-------\n",
        "\n",
        "https://www.youtube.com/watch?v=LsK-xG1cLYA\n"
      ]
    }
  ]
}